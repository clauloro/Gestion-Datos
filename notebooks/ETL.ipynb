{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL: Extracci√≥n desde Azure y Carga en SQL Server Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script realiza un proceso ETL autom√°tico extrayendo datos desde Azure SQL Database, transform√°ndolos en Pandas y carg√°ndolos en SQL Server Local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias\n",
    "import pyodbc  # Para conectar con bases de datos SQL Server\n",
    "import pandas as pd  # Para manipulaci√≥n de datos\n",
    "import numpy as np  # Para manejo de valores num√©ricos\n",
    "import os  # Para trabajar con archivos y directorios\n",
    "import warnings  # Para ocultar warnings innecesarios\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Ocultar warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicaci√≥n:**\n",
    "\n",
    "pyodbc ‚Üí Conexi√≥n con bases de datos SQL Server.\n",
    "\n",
    "pandas ‚Üí Manipulaci√≥n y limpieza de datos.\n",
    "\n",
    "numpy ‚Üí Optimizaci√≥n de datos num√©ricos.\n",
    "\n",
    "os ‚Üí Gesti√≥n de archivos.\n",
    "\n",
    "warnings ‚Üí Para suprimir advertencias innecesarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Configuraci√≥n de Conexiones a Bases de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Conexi√≥n a Azure SQL\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\n",
    "\n",
    "# üîπ Conexi√≥n a SQL Server Local\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicaci√≥n:**\n",
    "\n",
    "Se definen las credenciales de conexi√≥n tanto para Azure SQL como para SQL Server Local.\n",
    "\n",
    "ActiveDirectoryInteractive se usa para autenticaci√≥n en Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Definici√≥n de Tablas y Archivos SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Ubicaci√≥n de los archivos SQL con las consultas de extracci√≥n\n",
    "query_folder = \"DATA/data/Dataware house\"\n",
    "queries = {\n",
    "    \"dim_geo\": \"dim_geo.sql\",\n",
    "    \"dim_product\": \"dim_product.sql\",\n",
    "    \"dim_time\": \"dim_date.sql\",   # Corregido: en la imagen es dim_date.sql, no dim_time.sql\n",
    "    \"dim_client\": \"dim_client.sql\",\n",
    "    \"fact_sales\": \"dim_fact.sql\"  # Corregido: en la imagen es dim_fact.sql, no load_fact.sql\n",
    "}\n",
    "\n",
    "# üìå Definir claves primarias para cada tabla\n",
    "primary_keys = {\n",
    "    \"fact_sales\": [\"CODE\"],\n",
    "    \"dim_client\": [\"Customer_ID\"],\n",
    "    \"dim_geo\": [\"TIENDA_ID\"],\n",
    "    \"dim_product\": [\"Id_Producto\"],\n",
    "    \"dim_time\": [\"Fecha\"]\n",
    "}\n",
    "\n",
    "# üìå Definir claves for√°neas\n",
    "foreign_keys = {\n",
    "    \"fact_sales\": {\n",
    "        \"Customer_ID\": \"dim_client(Customer_ID)\",\n",
    "        \"TIENDA_ID\": \"dim_geo(TIENDA_ID)\",\n",
    "        \"Id_Producto\": \"dim_product(Id_Producto)\",\n",
    "        \"Sales_Date\": \"dim_time(Fecha)\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicaci√≥n:**\n",
    "\n",
    "queries ‚Üí Diccionario con las rutas de los archivos SQL que contienen las consultas de extracci√≥n.\n",
    "\n",
    "primary_keys ‚Üí Diccionario con las claves primarias de cada tabla.\n",
    "\n",
    "foreign_keys ‚Üí Diccionario con las relaciones entre tablas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Funci√≥n para Crear Tablas en SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_sql(table_name, df):\n",
    "    # Mapeo espec√≠fico de columnas de fecha para cada tabla\n",
    "    date_columns = {\n",
    "        \"dim_client\": [\"Fecha_nacimiento\"],\n",
    "        \"dim_time\": [\"InicioMes\", \"FinMes\", \"Fecha\"],\n",
    "        \"fact_sales\": [\"DATE_ULTIMA_REVISION\", \"Logistic_date\", \"Prod_date\", \"Sales_Date\"]\n",
    "    }\n",
    "    \n",
    "    col_defs = []\n",
    "    for col in df.columns:\n",
    "        # Verificar si la columna est√° en el mapeo manual de fechas\n",
    "        if table_name in date_columns and col in date_columns[table_name]:\n",
    "            col_defs.append(f'[{col}] DATE')\n",
    "        # Detecci√≥n autom√°tica de tipos para otras columnas\n",
    "        elif np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            col_defs.append(f'[{col}] DATE')\n",
    "        elif df[col].dtype == np.float32:\n",
    "            col_defs.append(f'[{col}] FLOAT')\n",
    "        elif df[col].dtype == np.int32:\n",
    "            col_defs.append(f'[{col}] INT')\n",
    "        else:\n",
    "            # Para columnas de texto, ajustamos el tama√±o seg√∫n los datos\n",
    "            max_len = df[col].astype(str).str.len().max()\n",
    "            varchar_size = min(2000, max(1, int(max_len * 1.3)))  # Buffer del 30% con m√°ximo 2000\n",
    "            col_defs.append(f'[{col}] NVARCHAR({varchar_size})')\n",
    "\n",
    "    # Agregar clave primaria si existe\n",
    "    pk = \", PRIMARY KEY (\" + \", \".join(primary_keys[table_name]) + \")\" if table_name in primary_keys else \"\"\n",
    "    \n",
    "    # Agregar claves for√°neas si existen\n",
    "    fk = \"\"\n",
    "    if table_name in foreign_keys:\n",
    "        for col, ref in foreign_keys[table_name].items():\n",
    "            fk += f\", FOREIGN KEY ({col}) REFERENCES {ref}\"\n",
    "\n",
    "    return f\"CREATE TABLE {table_name} ({', '.join(col_defs)}{pk}{fk});\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicaci√≥n:**\n",
    "\n",
    "Genera una consulta CREATE TABLE din√°micamente.\n",
    "\n",
    "Detecta tipos de datos (DATE, FLOAT, INT, NVARCHAR).\n",
    "\n",
    "Incluye claves primarias y for√°neas autom√°ticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Funci√≥n para Eliminar Tablas en Orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables_in_order(cursor, conn):\n",
    "    drop_order = [\"fact_sales\", \"dim_time\", \"dim_product\", \"dim_geo\", \"dim_client\"]\n",
    "    for table in drop_order:\n",
    "        check_exists_query = f\"IF OBJECT_ID('{table}', 'U') IS NOT NULL DROP TABLE {table};\"\n",
    "        try:\n",
    "            cursor.execute(check_exists_query)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al eliminar la tabla {table}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicaci√≥n:**\n",
    "\n",
    "Elimina las tablas en el orden correcto para evitar problemas con claves for√°neas.\n",
    "\n",
    "Verifica si la tabla existe antes de eliminarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ejecuci√≥n del Proceso ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexiones establecidas correctamente.\n",
      "Procesando: dim_geo\n",
      "Procesando: dim_product\n",
      "Procesando: dim_time\n",
      "Procesando: dim_client\n",
      "Procesando: fact_sales\n",
      "ETL COMPLETADO.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Conectar a Azure y SQL Server Local\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "    conn_local = pyodbc.connect(local_conn_str)\n",
    "    print(\"Conexiones establecidas correctamente.\")\n",
    "\n",
    "    with conn_local.cursor() as cursor:\n",
    "        drop_tables_in_order(cursor, conn_local)\n",
    "\n",
    "    # Procesar cada tabla\n",
    "    for table_name, file in queries.items():\n",
    "        print(f\"Procesando: {table_name}\")\n",
    "        query_path = os.path.join(query_folder, file)\n",
    "\n",
    "        with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_query = f.read()\n",
    "\n",
    "        # Extraer datos desde Azure\n",
    "        df = pd.read_sql(sql_query, conn_azure)\n",
    "\n",
    "        # Verificar si la tabla est√° vac√≠a\n",
    "        if df.empty:\n",
    "            print(f\"La tabla {table_name} no devolvi√≥ datos.\")\n",
    "            continue\n",
    "\n",
    "        # Conversi√≥n de tipos de datos\n",
    "        df = df.fillna(0)\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "        # Crear la tabla en SQL Server Local\n",
    "        with conn_local.cursor() as cursor:\n",
    "            create_sql = create_table_sql(table_name, df)\n",
    "            cursor.execute(create_sql)\n",
    "            conn_local.commit()\n",
    "\n",
    "            # Insertar datos\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error en el proceso ETL: {e}\")\n",
    "\n",
    "finally:\n",
    "    conn_azure.close()\n",
    "    conn_local.close()\n",
    "\n",
    "print(\"ETL COMPLETADO.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicaci√≥n:**\n",
    "\n",
    "Extrae datos de Azure SQL.\n",
    "\n",
    "Convierte tipos de datos antes de insertarlos.\n",
    "\n",
    "Carga los datos en SQL Server Local.\n",
    "\n",
    "Cierra conexiones despu√©s de finalizar el proceso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
